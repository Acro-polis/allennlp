{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from pytorch_pretrained_bert.modeling import BertConfig, BertModel\n",
    "\n",
    "from allennlp.common.testing import ModelTestCase\n",
    "from allennlp.data.dataset import Batch\n",
    "from allennlp.data.fields import TextField, ListField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.token_indexers.wordpiece_indexer import PretrainedBertIndexer\n",
    "from allennlp.data.tokenizers import WordTokenizer\n",
    "from allennlp.data.tokenizers.word_splitter import BertBasicWordSplitter\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.modules.token_embedders.bert_token_embedder import BertEmbedder\n",
    "from allennlp.common.testing.test_case import AllenNlpTestCase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = AllenNlpTestCase.FIXTURES_ROOT / 'bert' / 'vocab.txt'\n",
    "print(vocab_path)\n",
    "token_indexer = PretrainedBertIndexer(str(vocab_path))\n",
    "\n",
    "config_path = AllenNlpTestCase.FIXTURES_ROOT / 'bert' / 'config.json'\n",
    "config = BertConfig(str(config_path))\n",
    "bert_model = BertModel(config)\n",
    "token_embedder = BertEmbedder(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordTokenizer(word_splitter=BertBasicWordSplitter())\n",
    "\n",
    "#            2   3    4   3     5     6   8      9    2   14   12\n",
    "sentence1 = \"the quickest quick brown fox jumped over the lazy dog\"\n",
    "tokens1 = tokenizer.tokenize(sentence1)\n",
    "\n",
    "#            2   3     5     6   8      9    2  15 10 11 14   1\n",
    "sentence2 = \"the quick brown fox jumped over the laziest lazy elmo\"\n",
    "tokens2 = tokenizer.tokenize(sentence2)\n",
    "\n",
    "vocab = Vocabulary()\n",
    "\n",
    "\n",
    "\n",
    "instance1 = Instance({\"tokens\": TextField(tokens1, {\"bert\": token_indexer})})\n",
    "instance2 = Instance({\"tokens\": TextField(tokens2, {\"bert\": token_indexer})})\n",
    "\n",
    "batch = Batch([instance1, instance2])\n",
    "batch.index_instances(vocab)\n",
    "\n",
    "padding_lengths = batch.get_padding_lengths()\n",
    "tensor_dict = batch.as_tensor_dict(padding_lengths)\n",
    "tokens = tensor_dict[\"tokens\"]\n",
    "\n",
    "# 16 = [CLS], 17 = [SEP]\n",
    "assert tokens[\"bert\"].tolist() == [\n",
    "        [16, 2, 3, 4, 3, 5, 6, 8, 9, 2, 14, 12, 17, 0],\n",
    "        [16, 2, 3, 5, 6, 8, 9, 2, 15, 10, 11, 14, 1, 17]\n",
    "]\n",
    "\n",
    "assert tokens[\"bert-offsets\"].tolist() == [\n",
    "        [1, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
    "        [1, 2, 3, 4, 5, 6, 7, 10, 11, 12]\n",
    "]\n",
    "\n",
    "# No offsets, should get 14 vectors back ([CLS] + 12 token wordpieces + [SEP])\n",
    "bert_vectors = token_embedder(tokens[\"bert\"])\n",
    "assert list(bert_vectors.shape) == [2, 14, 12]\n",
    "\n",
    "# Offsets, should get 10 vectors back.\n",
    "bert_vectors = token_embedder(tokens[\"bert\"], offsets=tokens[\"bert-offsets\"])\n",
    "assert list(bert_vectors.shape) == [2, 10, 12]\n",
    "\n",
    "## Now try top_layer_only = True\n",
    "tlo_embedder = BertEmbedder(bert_model, top_layer_only=True)\n",
    "bert_vectors = tlo_embedder(tokens[\"bert\"])\n",
    "assert list(bert_vectors.shape) == [2, 14, 12]\n",
    "\n",
    "bert_vectors = tlo_embedder(tokens[\"bert\"], offsets=tokens[\"bert-offsets\"])\n",
    "assert list(bert_vectors.shape) == [2, 10, 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
